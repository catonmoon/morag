"""
title: Morag RAG
description: –ì–∏–±—Ä–∏–¥–Ω—ã–π RAG –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (Markdown / Confluence)
version: 0.2.0
"""
from __future__ import annotations

import base64
import hashlib
import json
import os
import requests
from itertools import groupby
from typing import Any, Dict, Generator, Iterator, List, Union

import numpy as np
from pydantic import BaseModel

_MD5_MOD = 4_294_967_295  # DO NOT CHANGE ‚Äî –ª–æ–º–∞–µ—Ç –∏–Ω–¥–µ–∫—Å


class Pipeline:
    class Valves(BaseModel):
        QDRANT_URL: str
        QDRANT_COLLECTION: str
        QDRANT_NUM_RESULTS: int
        NEIGHBOR_WINDOW: int

        SPARSE_EMBED_URL: str
        DENSE_EMBED_URL: str

        # –û—Å–Ω–æ–≤–Ω–∞—è LLM (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç)
        LLM_URL: str
        LLM_MODEL: str
        LLM_API_KEY: str
        LLM_TEMPERATURE: float

        # LLM –¥–ª—è reranker (–±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä)
        FILTER_MODEL_URL: str
        FILTER_MODEL: str
        FILTER_API_KEY: str
        FILTER_MAX_TOKENS: int
        FILTER_TEMPERATURE: float

        # LLM –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è intent –∏–∑ –¥–∏–∞–ª–æ–≥–∞
        INTENT_MODEL_URL: str
        INTENT_MODEL: str
        INTENT_API_KEY: str

    def __init__(self):
        self.valves = self.Valves(
            QDRANT_URL=os.getenv('QDRANT_URL', 'http://qdrant:6333'),
            QDRANT_COLLECTION=os.getenv('QDRANT_COLLECTION', 'chunks'),
            QDRANT_NUM_RESULTS=int(os.getenv('QDRANT_NUM_RESULTS', '50')),
            NEIGHBOR_WINDOW=int(os.getenv('NEIGHBOR_WINDOW', '1')),

            SPARSE_EMBED_URL=os.getenv('SPARSE_EMBED_URL', 'http://embedder-gte:8081'),
            DENSE_EMBED_URL=os.getenv('DENSE_EMBED_URL', 'http://embedder-frida:8082'),

            LLM_URL=os.getenv('LLM_URL', 'http://localhost:11434/v1'),
            LLM_MODEL=os.getenv('LLM_MODEL', 'qwen2.5:7b'),
            LLM_API_KEY=os.getenv('LLM_API_KEY', 'ollama'),
            LLM_TEMPERATURE=float(os.getenv('LLM_TEMPERATURE', '0.1')),

            FILTER_MODEL_URL=os.getenv('FILTER_MODEL_URL', os.getenv('LLM_URL', 'http://localhost:11434/v1')),
            FILTER_MODEL=os.getenv('FILTER_MODEL', os.getenv('LLM_MODEL', 'qwen2.5:7b')),
            FILTER_API_KEY=os.getenv('FILTER_API_KEY', os.getenv('LLM_API_KEY', 'ollama')),
            FILTER_MAX_TOKENS=int(os.getenv('FILTER_MAX_TOKENS', '50')),
            FILTER_TEMPERATURE=float(os.getenv('FILTER_TEMPERATURE', '0.1')),

            INTENT_MODEL_URL=os.getenv('INTENT_MODEL_URL', os.getenv('LLM_URL', 'http://localhost:11434/v1')),
            INTENT_MODEL=os.getenv('INTENT_MODEL', os.getenv('LLM_MODEL', 'qwen2.5:7b')),
            INTENT_API_KEY=os.getenv('INTENT_API_KEY', os.getenv('LLM_API_KEY', 'ollama')),
        )

    def pipe(
        self,
        user_message: str,
        model_id: str,
        messages: List[Dict],
        body: Dict,
    ) -> Union[str, Generator, Iterator]:
        # 1. –ò–∑–≤–ª–µ—á—å intent
        intent = self._extract_intent(messages)
        yield self._emit_status('üîé', intent, False)

        # 2. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        chunks = self._search(intent, self.valves.QDRANT_NUM_RESULTS)

        # 3. –†–∞—Å—à–∏—Ä–∏—Ç—å —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
        if self.valves.NEIGHBOR_WINDOW > 0 and chunks:
            chunks = self._expand_neighbors(chunks, self.valves.NEIGHBOR_WINDOW)

        yield self._emit_status('üîç', f'–§–∏–ª—å—Ç—Ä—É—é {len(chunks)} —á–∞–Ω–∫–æ–≤...', False)

        # 4. Reranker: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä (–ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É)
        yield '<think>'
        result_chunks: list[dict] = []
        chunks_by_doc = sorted(chunks, key=lambda x: x['doc_id'])
        for _, group_iter in groupby(chunks_by_doc, key=lambda x: x['doc_id']):
            for chunk in group_iter:
                answer = self._filter_chunk(intent, chunk)
                if not answer.startswith('0'):
                    result_chunks.append(chunk)
                    comment = answer.split('|', 1)[1].strip() if '|' in answer else answer.strip()
                    doc_name = chunk['path'].split('/')[-1]
                    yield f'[{doc_name}]: ‚úî {comment}\n'
        yield '</think>'

        if not result_chunks:
            yield self._emit_status('‚ùå', '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ', True)
            yield '–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.'
            return

        yield self._emit_status('‚úÖ', f'–ù–∞–π–¥–µ–Ω–æ {len(result_chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤', True)

        # Emit citations
        for chunk in result_chunks:
            yield self._emit_source(chunk['path'], chunk['text'])

        # 5. –°—Ç—Ä–∏–º–∏–Ω–≥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        context = self._build_context(result_chunks)
        yield from self._stream_answer(messages, context)

    # ‚îÄ‚îÄ Intent extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _extract_intent(self, messages: List[dict]) -> str:
        """–ï—Å–ª–∏ –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å ‚Äî –≤–µ—Ä–Ω—É—Ç—å –∫–∞–∫ –µ—Å—Ç—å. –ò–Ω–∞—á–µ ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ LLM."""
        if len(messages) == 1:
            return messages[0].get('content', '').strip()

        dialog = '\n'.join(
            f"{'User' if m['role'] == 'user' else 'Assistant'}: {m.get('content', '').strip()}"
            for m in messages if m['role'] in ('user', 'assistant')
        )
        prompt = (
            '–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.\n'
            '–ü—Ä–æ—á–∏—Ç–∞–π –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å—É—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —Å–µ–π—á–∞—Å —Ö–æ—á–µ—Ç —É–∑–Ω–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n'
            '–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ, –∫–∞–∫ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç".\n\n'
            f'–î–∏–∞–ª–æ–≥:\n{dialog}\n\n'
            '–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:'
        )
        return self._llm_complete(
            self.valves.INTENT_MODEL_URL, self.valves.INTENT_MODEL, self.valves.INTENT_API_KEY,
            [{'role': 'user', 'content': prompt}],
            temperature=0.1,
        ).strip()

    # ‚îÄ‚îÄ Reranker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _filter_chunk(self, query: str, chunk: dict) -> str:
        prompt = (
            f'–¢—ã —Ñ–∏–ª—å—Ç—Ä —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å: "{query}"\n\n'
            f'–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞:\n{chunk["text"]}\n\n'
            f'–ö–æ–Ω—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞:\n{chunk["context"]}\n\n'
            f'–ü—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞: {chunk["path"]}\n\n'
            '–ï—Å–ª–∏ —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–Ω–æ—Å—è—â—É—é—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É, –≤–µ—Ä–Ω–∏:\n'
            '1 | <2-4 —Å–ª–æ–≤–∞: –∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ>\n\n'
            '–ï—Å–ª–∏ —á–∞–Ω–∫ –ù–ï —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –≤–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ:\n'
            '0\n\n'
            '–í–ê–ñ–ù–û: –¢–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ.'
        )
        return self._llm_complete(
            self.valves.FILTER_MODEL_URL, self.valves.FILTER_MODEL, self.valves.FILTER_API_KEY,
            [{'role': 'user', 'content': prompt}],
            temperature=self.valves.FILTER_TEMPERATURE,
            max_tokens=self.valves.FILTER_MAX_TOKENS,
        )

    # ‚îÄ‚îÄ LLM helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _llm_complete(
        self, url: str, model: str, api_key: str,
        messages: list, temperature: float = 0.1, max_tokens: int | None = None,
    ) -> str:
        payload: dict = {'model': model, 'messages': messages, 'temperature': temperature}
        if max_tokens:
            payload['max_tokens'] = max_tokens
        resp = requests.post(
            f'{url.rstrip("/")}/chat/completions',
            headers={'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'},
            json=payload,
            timeout=60,
        )
        resp.raise_for_status()
        return resp.json()['choices'][0]['message']['content']

    def _stream_answer(self, messages: list, context: str) -> Generator:
        augmented = messages + [{'role': 'user', 'content': context}]
        payload = {
            'model': self.valves.LLM_MODEL,
            'messages': augmented,
            'temperature': self.valves.LLM_TEMPERATURE,
            'stream': True,
        }
        resp = requests.post(
            f'{self.valves.LLM_URL.rstrip("/")}/chat/completions',
            headers={
                'Authorization': f'Bearer {self.valves.LLM_API_KEY}',
                'Content-Type': 'application/json',
            },
            json=payload,
            stream=True,
            timeout=120,
        )
        resp.raise_for_status()
        for line in resp.iter_lines(decode_unicode=True):
            if not line or not line.startswith('data: '):
                continue
            data_str = line[6:]
            if data_str == '[DONE]':
                break
            try:
                data = json.loads(data_str)
                content = data['choices'][0]['delta'].get('content') or ''
                if content:
                    yield content
            except Exception:
                continue

    @staticmethod
    def _build_context(chunks: list) -> str:
        parts = [
            f'–ù–∞—á–∞–ª–æ —á–∞–Ω–∫–∞ ‚Ññ{i}\n'
            f'–ü—É—Ç—å: {c["path"]}\n'
            f'–ö–æ–Ω—Ç–µ–∫—Å—Ç: {c["context"]}\n'
            f'–¢–µ–∫—Å—Ç: {c["text"]}\n'
            f'–î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏: {c["updated_at"]}\n'
            f'–ö–æ–Ω–µ—Ü —á–∞–Ω–∫–∞ ‚Ññ{i}'
            for i, c in enumerate(chunks)
        ]
        return '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n\n' + '\n\n'.join(parts)

    # ‚îÄ‚îÄ Embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _embed_dense(self, text: str) -> list:
        payload = {'input': f'search_query: {text}', 'encoding_format': 'base64'}
        resp = requests.post(f'{self.valves.DENSE_EMBED_URL}/v1/embeddings', json=payload, timeout=30)
        resp.raise_for_status()
        b64 = resp.json()['data'][0]['embedding']
        return np.frombuffer(base64.b64decode(b64), dtype=np.float32).tolist()

    def _embed_sparse(self, text: str) -> tuple[list, list]:
        resp = requests.post(f'{self.valves.SPARSE_EMBED_URL}/encode', json={'text': text}, timeout=30)
        resp.raise_for_status()
        token_weights = resp.json()['token_weights'][0]
        return _sparse_dict_to_indices_values(token_weights)

    # ‚îÄ‚îÄ Qdrant search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _search(self, text: str, limit: int) -> list[dict]:
        dense = self._embed_dense(text)
        indices, values = self._embed_sparse(text)

        payload = {
            'prefetch': [
                {'query': {'indices': indices, 'values': values}, 'using': 'keywords', 'limit': limit * 2},
                {'query': dense, 'using': 'full', 'limit': limit * 2},
            ],
            'query': {'fusion': 'rrf'},
            'limit': limit,
            'with_payload': True,
        }
        url = f'{self.valves.QDRANT_URL}/collections/{self.valves.QDRANT_COLLECTION}/points/query'
        resp = requests.post(url, json=payload, timeout=30)
        resp.raise_for_status()
        points = resp.json().get('result', {}).get('points', [])
        return [_point_to_chunk(p) for p in points]

    def _expand_neighbors(self, chunks: list[dict], window: int) -> list[dict]:
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ (¬±window –ø–æ order –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ doc_id)."""
        existing_ids: set[str] = {c['chunk_id'] for c in chunks}
        by_doc: dict[str, set[int]] = {}
        for c in chunks:
            by_doc.setdefault(c['doc_id'], set()).add(c['order'])

        extra: list[dict] = []
        for doc_id, orders in by_doc.items():
            for order in list(orders):
                for delta in range(-window, window + 1):
                    if delta == 0:
                        continue
                    neighbor_order = order + delta
                    if neighbor_order < 0 or neighbor_order in orders:
                        continue
                    chunk = self._fetch_chunk_by_order(doc_id, neighbor_order)
                    if chunk and chunk['chunk_id'] not in existing_ids:
                        extra.append(chunk)
                        existing_ids.add(chunk['chunk_id'])
                        orders.add(neighbor_order)

        all_chunks = chunks + extra
        return sorted(all_chunks, key=lambda x: (x['doc_id'], x['order']))

    def _fetch_chunk_by_order(self, doc_id: str, order: int) -> dict | None:
        payload = {
            'filter': {
                'must': [
                    {'key': 'doc_id', 'match': {'value': doc_id}},
                    {'key': 'order', 'match': {'value': order}},
                ]
            },
            'limit': 1,
            'with_payload': True,
        }
        url = f'{self.valves.QDRANT_URL}/collections/{self.valves.QDRANT_COLLECTION}/points/scroll'
        resp = requests.post(url, json=payload, timeout=10)
        resp.raise_for_status()
        points = resp.json().get('result', {}).get('points', [])
        if not points:
            return None
        p = points[0]
        chunk = _point_to_chunk(p)
        chunk['score'] = 0.0
        return chunk

    # ‚îÄ‚îÄ Open WebUI events ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @staticmethod
    def _emit_status(emoji: str, text: str, done: bool = False) -> dict[str, Any]:
        return {'event': {'type': 'status', 'data': {'description': f'{emoji} {text}', 'done': done}}}

    @staticmethod
    def _emit_source(name: str, content: str) -> dict[str, Any]:
        return {
            'event': {
                'type': 'citation',
                'data': {
                    'document': [content],
                    'metadata': [{'source': name}],
                    'source': {'name': name},
                },
            }
        }


# ‚îÄ‚îÄ Module-level helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _sparse_dict_to_indices_values(sparse_dict: dict) -> tuple[list, list]:
    """MD5(word) % 4_294_967_295 ‚Üí –∏–Ω–¥–µ–∫—Å. –ù–ï –ú–ï–ù–Ø–¢–¨ ‚Äî –ª–æ–º–∞–µ—Ç –∏–Ω–¥–µ–∫—Å."""
    index_weight: dict[int, float] = {}
    for word, weight in sparse_dict.items():
        idx = int(hashlib.md5(word.encode('utf-8')).hexdigest(), 16) % _MD5_MOD
        if idx in index_weight:
            index_weight[idx] = max(index_weight[idx], weight)
        else:
            index_weight[idx] = weight
    return list(index_weight.keys()), list(index_weight.values())


def _point_to_chunk(p: dict) -> dict:
    payload = p.get('payload', {})
    return {
        'chunk_id': str(p['id']),
        'doc_id': payload.get('doc_id', ''),
        'path': payload.get('path', ''),
        'order': payload.get('order', 0),
        'total': payload.get('total', 0),
        'text': payload.get('text', ''),
        'context': payload.get('context', ''),
        'updated_at': payload.get('updated_at', ''),
        'creator': payload.get('creator', ''),
        'score': p.get('score', 0.0),
    }
